{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:16.299633Z",
     "start_time": "2022-05-13T09:39:13.575383Z"
    }
   },
   "source": [
    "# This code now can work well, it is suitable for both MNIST and fashion-MNIST\n",
    "from __future__ import print_function\n",
    "import pandas as pd\n",
    "import keras\n",
    "# from keras.datasets import mnist\n",
    "from keras.datasets import fashion_mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPooling2D\n",
    "from keras.constraints import max_norm\n",
    "from keras import backend as k\n",
    "from keras.utils import np_utils\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:16.553662Z",
     "start_time": "2022-05-13T09:39:16.300752Z"
    }
   },
   "source": [
    "#load mnist dataset\n",
    "# (X_train, y_train), (X_test, y_test) = mnist.load_data() \n",
    "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data() \n",
    "#everytime loading data won't be so easy :)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:17.167245Z",
     "start_time": "2022-05-13T09:39:16.554782Z"
    }
   },
   "source": [
    "#visualising first 9 data from training dataset\n",
    "fig = plt.figure()\n",
    "for i in range(9):\n",
    "  plt.subplot(3,3,i+1)\n",
    "  plt.tight_layout()\n",
    "  plt.imshow(X_train[i], cmap='gray', interpolation='none')\n",
    "  plt.title(\"Class: {}\".format(y_train[i]))\n",
    "  plt.xticks([])\n",
    "  plt.yticks([])\n",
    "fig"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:17.172846Z",
     "start_time": "2022-05-13T09:39:17.168926Z"
    }
   },
   "source": [
    "# let's print the actual data shape before we reshape and normalize\n",
    "print(\"X_train shape\", X_train.shape)\n",
    "print(\"y_train shape\", y_train.shape)\n",
    "print(\"X_test shape\", X_test.shape)\n",
    "print(\"y_test shape\", y_test.shape)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:17.185002Z",
     "start_time": "2022-05-13T09:39:17.173911Z"
    }
   },
   "source": [
    "#input image size 28*28\n",
    "img_rows , img_cols = 28, 28"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:17.266202Z",
     "start_time": "2022-05-13T09:39:17.186684Z"
    }
   },
   "source": [
    "#reshaping\n",
    "#this assumes our data format\n",
    "#For 3D data, \"channels_last\" assumes (conv_dim1, conv_dim2, conv_dim3, channels) while \n",
    "#\"channels_first\" assumes (channels, conv_dim1, conv_dim2, conv_dim3).\n",
    "if k.image_data_format() == 'channels_first':\n",
    "    X_train = X_train.reshape(X_train.shape[0], 1, img_rows, img_cols)\n",
    "    X_test = X_test.reshape(X_test.shape[0], 1, img_rows, img_cols)\n",
    "    input_shape = (1, img_rows, img_cols)\n",
    "else:\n",
    "    X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "    X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "    input_shape = (img_rows, img_cols, 1)\n",
    "#more reshaping\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print('X_train shape:', X_train.shape)\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:17.271241Z",
     "start_time": "2022-05-13T09:39:17.267323Z"
    }
   },
   "source": [
    "print(np.unique(y_train, return_counts=True))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:17.274602Z",
     "start_time": "2022-05-13T09:39:17.272362Z"
    }
   },
   "source": [
    "#set number of categories\n",
    "num_category = 10"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:17.280763Z",
     "start_time": "2022-05-13T09:39:17.275723Z"
    }
   },
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_train = keras.utils.np_utils.to_categorical(y_train, num_category)\n",
    "y_test = keras.utils.np_utils.to_categorical(y_test, num_category)\n",
    "y_train[0]"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:17.417402Z",
     "start_time": "2022-05-13T09:39:17.281883Z"
    }
   },
   "source": [
    "##model building\n",
    "model = Sequential()\n",
    "#convolutional layer with rectified linear unit activation\n",
    "model.add(Conv2D(3, kernel_size=(3, 3),\n",
    "                 activation='relu',\n",
    "                 input_shape=input_shape,\n",
    "                kernel_constraint=max_norm(1.)))\n",
    "#32 convolution filters used each of size 3x3\n",
    "#again\n",
    "# model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "#64 convolution filters used each of size 3x3\n",
    "#Choose the best features via pooling\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#randomly turn neurons on and off to improve convergence\n",
    "model.add(Dropout(0.25))\n",
    "#flatten since too many dimensions, we only want a classification output\n",
    "model.add(Flatten())\n",
    "#fully connected to get all relevant data\n",
    "model.add(Dense(100, activation='relu'))\n",
    "#one more dropout for convergence' sake :) \n",
    "model.add(Dropout(0.2))\n",
    "#output a softmax to squash the matrix into output probabilities\n",
    "model.add(Dense(num_category, activation='softmax'))\n",
    "#Adaptive learning rate (adaDelta) is a popular form of gradient descent rivaled only by adam and adagrad\n",
    "#categorical ce since we have multiple classes (10) \n",
    "model.compile(loss=keras.losses.categorical_crossentropy,\n",
    "              optimizer=tf.optimizers.Adam(learning_rate=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False),\n",
    "              metrics=['accuracy'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:17.424682Z",
     "start_time": "2022-05-13T09:39:17.418522Z"
    }
   },
   "source": [
    "def find_neighbour_C(C_in,lt_df):\n",
    "    tmp_arr = lt_df['Conductance'].values\n",
    "    tmp_arr = np.sort(np.append(tmp_arr,C_in))\n",
    "    ave_C = (tmp_arr[np.where(np.sort(tmp_arr)==C_in)[0]-1]+tmp_arr[np.where(np.sort(tmp_arr)==C_in)[0]+1])/2\n",
    "#     print('find_neighbour_C C_in',C_in)\n",
    "#     print(\"ave_C:\",ave_C,np.where(np.sort(tmp_arr)==C_in)[0])\n",
    "    if C_in > ave_C:\n",
    "        C_out = tmp_arr[np.where(np.sort(tmp_arr)==C_in)[0]+1]\n",
    "    else: \n",
    "        C_out = tmp_arr[np.where(np.sort(tmp_arr)==C_in)[0]-1]\n",
    "    \n",
    "    return C_out[0]\n",
    "\n",
    "\n",
    "\n",
    "def map_weights(w_pre,w_post,w_max,w_min,ltp_df,ltd_df):\n",
    "    \n",
    "    data_df = pd.concat([ltp_df,ltd_df],axis=0)\n",
    "    \n",
    "    #set Conductance range\n",
    "    C_max = np.sort(data_df['Conductance'].values)[-2]\n",
    "    C_min = np.sort(data_df['Conductance'].values)[1]\n",
    "#   print('C_max',C_max,'C_min',C_min)\n",
    "\n",
    "    # solve linear equations and get patameters in the map function\n",
    "    k = (C_max-C_min)/(w_max-w_min)\n",
    "    b = C_min - k * w_min\n",
    "    C_in = k * w_post + b\n",
    "#     print(C_in)\n",
    "    C_in = np.trunc(C_in*1000)/1000\n",
    "#     print('\\nmap_weights_C_in',C_in)\n",
    "    \n",
    "    ltp = w_post > w_pre\n",
    "#     print('w_pre:{},w_post:{},ltp:{}'.format(w_pre,w_post,ltp))\n",
    "    \n",
    "#     find_C_vect = np.vectorize(find_neighbour_C)\n",
    "#     find_C_vect.excluded.add(1)\n",
    "    \n",
    "    if ltp:\n",
    "        C_out = find_neighbour_C(C_in,ltp_df)\n",
    "    else:\n",
    "        C_out = find_neighbour_C(C_in,ltd_df)\n",
    "    \n",
    "    w_out = (C_out - b) / k\n",
    "#     print('mapped_weight:{}, error_percent:{}%'.format(w_out,(w_out-w_post)/w_post*100))\n",
    "    return C_out,w_out\n",
    "\n",
    "map_weights_vect = np.vectorize(map_weights,excluded=['w_max','w_min','ltp_df','ltd_df'])"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:39:17.513122Z",
     "start_time": "2022-05-13T09:39:17.425802Z"
    }
   },
   "source": [
    "#import device data\n",
    "data_path = './ltp-ltd.txt' #change file path\n",
    "data_df = pd.read_csv(data_path , names=['spike_nums','Conductance'], sep='\\t')\n",
    "idx = data_df['Conductance'].idxmax()\n",
    "ltp_df = data_df.iloc[:idx+1,:]\n",
    "ltd_df = data_df.iloc[idx:,:]\n",
    "plt.plot(data_df['spike_nums'].values,data_df['Conductance'].values,'ob')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:46:17.442922Z",
     "start_time": "2022-05-13T09:39:17.516481Z"
    },
    "scrolled": true
   },
   "source": [
    "batch_size = 128\n",
    "num_epoch = 100\n",
    "\n",
    "max_weight_list = []\n",
    "min_weight_list = []\n",
    "\n",
    "train_log = []\n",
    "test_acc_log = []\n",
    "test_loss_log = []\n",
    "\n",
    "C_weights_log= []\n",
    "C_bias_log = []\n",
    "\n",
    "#model training\n",
    "for i in range(num_epoch):\n",
    "\n",
    "    print('\\n\\nepoch: {}/{}\\n'.format(i + 1, num_epoch))\n",
    "    \n",
    "    #get weights before training\n",
    "    for layer in filter(lambda x: 'conv2d' in x.name, model.layers):\n",
    "        pre_weights = layer.get_weights()[0]\n",
    "        pre_bias = layer.get_weights()[1]\n",
    "#     print('pre_weights', pre_weights, '\\npre_bias', pre_bias)\n",
    "    \n",
    "    #fit model\n",
    "    model_log = model.fit(X_train,\n",
    "                          y_train,\n",
    "                          batch_size=batch_size,\n",
    "                          epochs=1,\n",
    "                          verbose=1,\n",
    "                          validation_data=(X_test, y_test))\n",
    "    train_log.append(model_log.history)\n",
    "    \n",
    "    test_score_original = model.evaluate(X_test, y_test, verbose=0)\n",
    "    print(\n",
    "        ' - Test_loss: {:4f} - Test_accuracy: {:4f}'\n",
    "        .format(test_score_original[0], test_score_original[1]))\n",
    "    \n",
    "    #get weights after training\n",
    "    for layer in filter(lambda x: 'conv2d' in x.name, model.layers):\n",
    "        post_weights = layer.get_weights()[0]\n",
    "        post_bias = layer.get_weights()[1]\n",
    "#     print('post_weights', post_weights, '\\npost_bias', post_bias)\n",
    "\n",
    "    max_weight_list.append([np.max(post_weights), np.max(post_bias)])\n",
    "    min_weight_list.append([np.min(post_weights), np.min(post_bias)])\n",
    "    \n",
    "    #mapping weights\n",
    "    C_weights, mapped_weights = map_weights_vect(w_pre=pre_weights,\n",
    "                                      w_post=post_weights,\n",
    "                                      w_max=1,\n",
    "                                      w_min=-1,\n",
    "                                      ltp_df=ltp_df,\n",
    "                                      ltd_df=ltd_df)\n",
    "    C_bias, mapped_bias = map_weights_vect(w_pre=pre_bias,\n",
    "                                   w_post=post_bias,\n",
    "                                   w_max=0.6,\n",
    "                                   w_min=-0.1,\n",
    "                                   ltp_df=ltp_df,\n",
    "                                   ltd_df=ltd_df)\n",
    "    C_weights_log.append(C_weights)\n",
    "    C_bias_log.append(C_bias)\n",
    "    \n",
    "    #set weights\n",
    "    for layer in filter(lambda x: 'conv2d' in x.name, model.layers):\n",
    "        layer.set_weights([mapped_weights, mapped_bias])\n",
    "\n",
    "    for layer in filter(lambda x: 'conv2d' in x.name, model.layers):\n",
    "        mapped_weights = layer.get_weights()[0]\n",
    "        mapped_bias = layer.get_weights()[1]\n",
    "        print('- mapping weights...')\n",
    "#     print('mapped_weights: \\n', mapped_weights, '\\nmapped_bias:\\n', mapped_bias)\n",
    "\n",
    "#     train_score = model.evaluate(X_train, y_train, verbose=0)\n",
    "    test_score_map = model.evaluate(X_test, y_test, verbose=0)\n",
    "    test_loss_log.append(test_score_map[0])\n",
    "    test_acc_log.append(test_score_map[1])\n",
    "    \n",
    "    print(\n",
    "        ' - Test_loss: {:4f} - Test_accuracy: {:4f}'\n",
    "        .format(test_score_map[0], test_score_map[1]))\n",
    "    \n",
    "    if test_score_map[1]-test_score_original[1]<-0.05:\n",
    "        break\n",
    "        "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "source": [
    "# post_weights"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "source": [
    "# mapped_weights"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "source": [
    "# (mapped_weights - post_weights)/post_weights"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:46:17.452653Z",
     "start_time": "2022-05-13T09:46:17.444267Z"
    },
    "scrolled": true
   },
   "source": [
    "# max_weight_list "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:46:17.465091Z",
     "start_time": "2022-05-13T09:46:17.453846Z"
    },
    "scrolled": true
   },
   "source": [
    "# min_weight_list"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:49:32.657072Z",
     "start_time": "2022-05-13T09:49:31.988721Z"
    }
   },
   "source": [
    "# plotting the metrics\n",
    "fig = plt.figure()\n",
    "# plt.subplot(2,1,1)\n",
    "# plt.plot(model_log.history['val_accuracy'])\n",
    "plt.plot(test_acc_log)\n",
    "plt.title('model accuracy on testing data')\n",
    "plt.ylabel('accuracy')\n",
    "plt.xlabel('epoch')\n",
    "plt.ylim(np.min(test_acc_log)-0.01,np.max(test_acc_log)+0.02)\n",
    "\n",
    "# plt.subplot(2,1,2)\n",
    "# plt.plot(test_loss_log)\n",
    "# plt.title('model loss on testing data')\n",
    "# plt.ylabel('loss')\n",
    "# plt.xlabel('epoch')\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('./cnn_with_device_fashion_mnist.png',dpi=600)\n",
    "plt.savefig('./cnn_with_device_fashion_mnist.svg')\n",
    "plt.show()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T09:46:17.667367Z",
     "start_time": "2022-05-13T09:46:17.614951Z"
    }
   },
   "source": [
    "#Save the model\n",
    "# serialize model to JSON\n",
    "model_digit_json = model.to_json()\n",
    "with open(\"model_digit.json\", \"w\") as json_file:\n",
    "    json_file.write(model_digit_json)\n",
    "# serialize weights to HDF5\n",
    "model.save_weights(\"model_digit.h5\")\n",
    "print(\"Saved model to disk\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2022-05-13T10:28:48.135981Z",
     "start_time": "2022-05-13T10:28:48.131457Z"
    }
   },
   "source": [
    "#save device weights and bias\n",
    "save_C_weights = np.array(C_weights_log)\n",
    "np.save('./Device_weights.npy',save_C_weights)\n",
    "save_C_bias = np.array(C_bias_log)\n",
    "np.save('./Device_bias.npy',save_C_bias)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "source": [
    "C_weight_final1 = save_C_weights[-1,:,:,:,0].reshape(3,3)\n",
    "C_weight_final2 = save_C_weights[-1,:,:,:,1].reshape(3,3)\n",
    "C_weight_final3 = save_C_weights[-1,:,:,:,2].reshape(3,3)\n",
    "print (C_weight_final1)\n",
    "plt.imshow(C_weight_final3)\n",
    "plt.title('C_weight_final3')\n",
    "plt.colorbar()"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "source": [
    "import numpy\n",
    "numpy.savetxt(\"D:\\Python\\My_Python\\deep-learning-cnn\\cnn_with_device_fashionMnist_zhufangduo\\\\test_acc_log.txt\", test_acc_log)\n",
    "numpy.savetxt(\"D:\\Python\\My_Python\\deep-learning-cnn\\cnn_with_device_fashionMnist_zhufangduo\\\\C_weight_final1.txt\",C_weight_final1)\n",
    "numpy.savetxt(\"D:\\Python\\My_Python\\deep-learning-cnn\\cnn_with_device_fashionMnist_zhufangduo\\\\C_weight_final2.txt\",C_weight_final2)\n",
    "numpy.savetxt(\"D:\\Python\\My_Python\\deep-learning-cnn\\cnn_with_device_fashionMnist_zhufangduo\\\\C_weight_final3.txt\",C_weight_final3)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "source": [
    "print(model)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "source": [
    "from keras.models import Model\n",
    "\n",
    "sample = X_test[1,:,:,:].reshape(1,28,28,1)\n",
    "conv2d_layer_model = Model(inputs=model.input, outputs=model.layers[0].output)\n",
    "pooling_layer_model = Model(inputs=model.input, outputs=model.layers[1].output)\n",
    "\n",
    "conv2d_feature=conv2d_layer_model.predict(sample)\n",
    "pooling_feature = pooling_layer_model.predict(sample)\n",
    "sample2 = X_test[1,:,:,:].reshape(28,28)\n",
    "plt.imshow(sample2,cmap='gray')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "source": [
    "#conv2d layer output\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    figg = conv2d_feature[:,:,:,i].reshape(26,26)\n",
    "    plt.imshow(figg, interpolation='none')\n",
    "    "
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "source": [
    "#pooling layer output\n",
    "for i in range(3):\n",
    "    plt.subplot(1,3,i+1)\n",
    "    figg = pooling_feature[:,:,:,i].reshape(13,13)\n",
    "    plt.imshow(figg, interpolation='none')"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
